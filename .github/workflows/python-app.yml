name: Proxy Scraper Automation

on:
  schedule:
    - cron: '0 */5 * * *'  # Runs every 5 hours
  workflow_dispatch:  # Allows manual triggering of the workflow

permissions:
  contents: write  # Allow pushing changes to the repository

jobs:
  scrape-and-clean:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Install dependencies
      run: |
        python3 -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run Proxy Scraper
      run: |
        python3 main.py  # Run your scraper script to update proxies.txt

    - name: Clean Proxies
      run: |
        python3 clean_proxies.py  # Run the cleaning script to create cleaned_proxies.txt

    - name: Commit and Push Updated Proxies
      run: |
        git config --global user.name "zebbern"
        git config --global user.email "zebbern@users.noreply.github.com"
        git add proxies.txt cleaned_proxies.txt
        git commit -m "Updated proxies.txt and cleaned_proxies.txt via automation"
        git push
